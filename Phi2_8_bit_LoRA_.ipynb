{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSTZ7g0jcw8P"
      },
      "source": [
        "#### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LkAdUuNZzjJi"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch transformers peft bitsandbytes datasets trl accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dp870C_kYzr4",
        "outputId": "2a197350-05c8-496a-a3b9-9f3d4d2bda66"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ2kqaWXzTdv"
      },
      "source": [
        "#### Auto-detect dtype based on GPU capability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVxm6xRXrb2Z",
        "outputId": "7b72b776-9567-4f27-87f5-981d30177036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
            "Compute Capability: 8.x\n",
            "Using bfloat16 (Ampere+ GPU detected)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_capability = torch.cuda.get_device_capability()[0]\n",
        "\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"Compute Capability: {gpu_capability}.x\")\n",
        "\n",
        "    # Ampere (RTX 30xx, A100) and newer (capability >= 8) support bf16 efficiently\n",
        "    # Older GPUs (T4, V100, RTX 20xx) should use fp16\n",
        "    if gpu_capability >= 8:\n",
        "        torch_dtype = torch.bfloat16\n",
        "        use_bf16 = True\n",
        "        use_fp16 = False\n",
        "        attn_implementation = \"flash_attention_2\"\n",
        "        print(\"Using bfloat16 (Ampere+ GPU detected)\")\n",
        "    else:\n",
        "        torch_dtype = torch.float16\n",
        "        use_bf16 = False\n",
        "        use_fp16 = True\n",
        "        attn_implementation = \"eager\"\n",
        "        print(\"Using float16 (Pre-Ampere GPU detected)\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P333JDc8Tgm3"
      },
      "source": [
        "#### Part 1: Loading model in 8-bit (8-bit Quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtXt3vPW-hVb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# 1. Define the Quantization configuration\n",
        "# This loads the weights in 8-bit integers\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "05cbefa87c934f858fdc61deb7748c4d",
            "280224b4a51041c2b82e64df75d01548",
            "1835ab7bf31c46b6a216861ba1b3b89e",
            "ad0aee949ed64422b589b4f3a81d458c",
            "89808e638937461299b4deac69e73d48",
            "341e7529e9c94584ae6d1272fe893a97",
            "219eb613a54f4ee895bc5cd473414d64",
            "d8bcaf0e21554558be486ec420a1e48f",
            "30d9f161e0ad4e2d8f5cae7e145eb859",
            "b27abc9b6230482ca5f9befdd3d094af",
            "748f789117ac4f8fac9c66b657bf073d",
            "2e06e4242f824ec8ab964ef807d74075",
            "a75e4207f350482d9ab1695f67515daf",
            "c2adadccb41d4c8d9f973909fcc0b31e",
            "a29f379698ed4ccb9ef4643100647951",
            "119af0989cdf492193838be86664b693",
            "74773c69626f4cc09416b2b3fe12e06b",
            "de25cb70e8744b27b9bb2ab4e2c7614f",
            "0bd085a3a80649dea4e8588978a8899b",
            "07c27ef7ceee4044bb0b83d687ceddf5",
            "06326ca35d1443328b3b1ca496c60ae7",
            "87223d3f1b1543bbb9aa08db0dd52605",
            "340ef2dee42b43c6b7b61fbe23ba28db",
            "9fe93592dc6a46418d7c5a7fa3a03de8",
            "e97a7d29b9b94048a601c3c6b515cb49",
            "d232a7ea09784afb8f2bf1b583e6edb0",
            "a6ac58570224433c871e7287f9ad0ad2",
            "552fc56b3e814aeca649f64cb8c25698",
            "1558d3aea1e44632bbea4831ef860325",
            "a19aaacc2c604b68ae882774fcff9c70",
            "78031b8b9c834d63bd74b0f368a2e738",
            "d0ceae2c3b084f12b170af152cf21c62",
            "230cb034434b4528a10062bca88f1c38",
            "115034fcd4ef41958030a39ca88715fa",
            "f30326dbe326434aac43042af5a8d120",
            "85182867e5b944d1b91f8ce4ee19e9a3",
            "9a73794ad1e144768e7f9468d7dc4f10",
            "a030c0480bbf4eaf9cf63dfd84449607",
            "48e24de5ce064f67a33d903b13d337f4",
            "94f0b2db578f4295a10402055c741145",
            "170792a9869840cc80bffdd0c351f2d9",
            "f94ac4d0ef7e4a60b9bd017e6fe07e71",
            "2d82a4fac6e74716a5717f2d7dfa7d08",
            "c9451ec291904c4dbbf14cf37b1b694c",
            "a84204830c9a4a9ba38e43a4028f5252",
            "8a00cb5cfd524a0bbcc191b85a0eb274",
            "e2bbe9311a6f44c0956187da4eec27ce",
            "fe37c85bfddc4b4da0f47a6e79e826fe",
            "4f3bc4b255cf4b958a9c05e852187f5f",
            "fcb4705c213d493582d1b1cb07cd3000",
            "165889709f514c56b97b625d025ef817",
            "89bcba6eebef4c0f8eb3cfe95ded1c97",
            "b35b242dcccc46e79c0ff8a872ef01ea",
            "eb45ea29b3d44d62982c00bc935eae45",
            "26df244d3d1f44e29453f7f90bbaf2fc",
            "b00e362ca9784a0b8c40d5f0a096d6a2",
            "d3bcd4f2cf0e49ee9c387c6a0c9dac45",
            "155bc3f47e4946a1bb615a8370d067ec",
            "1888936521294a60acfb4429cb87bffa",
            "75f4216c72aa426e9b7e6cb80a92fdb9",
            "d905a725eb9b4e929334fd00ade676c1",
            "f4b386469af44648ac9c9b30fc051d64",
            "cd7c19e6cb114444a0ba70353e35c7cd",
            "d686d38a009247699f96e294bfe7c6c7",
            "715c257cff134fffb76f2c5cfe12c99b",
            "5f8e13fd4d4b4558bb599b682498b229"
          ]
        },
        "collapsed": true,
        "id": "V4XnUxJ1Ubd1",
        "outputId": "3685d5a2-3739-4ec3-f0d5-fe04b38f1973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded: CodeGenTokenizerFast\n",
            "Vocab size: 50,257\n"
          ]
        }
      ],
      "source": [
        "# 2. Load Tokenizer\n",
        "model_id = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "3cd043b43e1343f489263519297fa96a",
            "5f1a132054d4405d9774484f7b160d30",
            "3661d1a2160d4e16a00347748cb452f0",
            "40a733e974c34e2c8685abae7c3a1cdf",
            "51467598712b4a8cb5aeb3de54c58799",
            "5172e446dc334bc9b281a0fb54db3140",
            "2fb4b9785fbb48539fe11aab92833e01",
            "47bc6a51a9244409886f8c0873123fec",
            "ca5c6d2ce1be405c8f2ed6ffa2abb94c",
            "4bd73de3bea64df98c667bd930921071",
            "4902b341df2441d3b33549896ae8b371",
            "4e2acb5610104b719bf2de649b097a21",
            "04aa48f8b3ec449ab616756c8dcdb9c4",
            "d4a65622a25f4788a1e2e009a4c32784",
            "709cde3acf62428f9b17086dd22d1cfb",
            "6d4618bdeab74e1b84e38d5d1b9d2d42",
            "08ff7c66ce784c30ac28c1ff8e670c4d",
            "183eb054746d44b09954c1d46a6fddb8",
            "19c5f8eeebba4eefb95903008d11331e",
            "d1fe47bcc7dc463ebc1d501f5454de11",
            "a13f776e4af9417b8fcb6f3315a1e38f",
            "b6d9a0667bea47ab844e7791d3183ca9",
            "2916da6bf0af4b389c2fa1f29f4b866b",
            "fee34f7af9bd446d9916521fc3d92ae2",
            "d65fd02b4f834befb49a0a0df1367323",
            "7545ae772d0647c2a3594e9a7a7ac51c",
            "f2370bd836bc47a585fe189da0442705",
            "c10d4c84471c42fd89a3fa303a3656e0",
            "7b06bd3b4a6140bdb8efae94192619e4",
            "6f212e3d99954850b01b0fc1e4ca3d55",
            "afd9097f0bdc439f8bd796803c3a24a2",
            "5352193043394037b3715d4cb53b6e0d",
            "99e2b9b7405c4fbbb8d552a4fca138be",
            "65713e3f3b7f41c79252cf9a6d6ced1c",
            "f5108ed7b19348a2b7d7070d3b56c5e5",
            "3b326a1137524563b6a0db87339c67cf",
            "2efd349922da449286c865f10c15e3b4",
            "ffcba03102a6433bafca8c653d1d1e28",
            "a05a0342812e44378cf7511ab1a94efd",
            "0a230ce5c7604b9d84ac543faf758563",
            "7c109fc168004acf948b7c7acfbbe9a7",
            "8d9fcf1058f146d28e71900d2c6dc4c3",
            "1c8ff0bdd3504c9693d1212f56b50884",
            "78de17e7123e432e82bfef4c82c44b88",
            "e1703c8488af4f94b9933e539159ac83",
            "5c99e481fcb64db8b4842e19d304c346",
            "0217cb3d2076462f93d9de01705c0547",
            "3229e142a97e4f89985bee87d5704cbf",
            "65577919fa3740b6abb05ecfd54301f1",
            "a0d78924473449d8b12b84cc25a391b2",
            "7c25496097614d45ae37efe073125689",
            "a53124eff12a458cbd480fd9c26dc2cf",
            "58cb2a7492eb4a518feb6a95f4e84cf1",
            "51fd020edfa4467faa27b8bb0f79676a",
            "f1f23b481e4a4638add4a5818a0dea74",
            "2956004c1aac428b981e06fa9bc6f24c",
            "ce6624ee6d1e46cda2f3912b89d5bf7d",
            "fee17b3816cd4ba387ad5ae541b4b883",
            "b5f2d6a25dcf406eb7a0b414b59b1c5f",
            "b3023185b6e84f8dbcd35eb630c0853f",
            "03d0861e5d604e9889ebb3169d3c2e19",
            "be54985a3bba43a09308e612b61ca07b",
            "2c5eb61cae6f467397b645c799227d25",
            "71da7064dfe94c7da9524ec7f5a76226",
            "31789c06f48b492dbde5228ab9bc5aec",
            "ecbdd965539d462198d56d91d386c13d",
            "8ece5a5138aa4f12bd42e6c67ff0de42",
            "7f3e549190a043adad3569534ce40497",
            "b48cd654fe9b414291d47e4e5367d3b5",
            "36a1b2aca1c54733b6d97d50084d40a4",
            "d739286cce804f93aac3dd73c7c67997",
            "2655fde5a4d34065bcba1ae22eacf5b3",
            "40ef6c7e611b40b79af6a045252a5a32",
            "9cff857a523944cd85ccfceea75e5ff1",
            "e1d883afcf3a4fc6b25c26e1c225788c",
            "0e881fdf6698499cb61091378e55190f",
            "91a9d7e35e724898a55e9405a695b5d3"
          ]
        },
        "collapsed": true,
        "id": "2QvO5dkqVCXd",
        "outputId": "fbf7350c-9844-46d9-ddfd-07760d98a939"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Memory footprint: 3.04 GB\n",
            "Model parameters: 2.8B\n"
          ]
        }
      ],
      "source": [
        "# 3. Load the model with Quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",          # Automatically maps layers to GPU/CPU\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"Model loaded. Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e9:.1f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YFGA-KDFntTK",
        "outputId": "ef936ac8-b724-4b42-f583-dc5d6b3575c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2560)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
            "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (rotary_emb): PhiRotaryEmbedding()\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HqbV516W4xE"
      },
      "source": [
        "#### Part 2: 8-bit model fine-tuning (Training) using LoRA\n",
        "\n",
        "An 8-bit model can't be trained directly (the weights are frozen). Therefore using PEFT (Parameter-Efficient Fine-Tuning) to attach small, trainable adapters (LoRA) on top of the frozen 8-bit weights is needed in order to train the quantized model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTmdWoy_VCtK",
        "outputId": "a39e43b9-92fe-40ed-be75-304ff8093105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 23,592,960 || all params: 2,803,276,800 || trainable%: 0.8416\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Prepare model for k-bit training (8-bit training in our case)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 2. Configure LoRA (Low-Rank Adaptation)\n",
        "# This adds trainable adapters to the model\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[                         # Phi-2 specific modules\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "93a41587130b4f479de8daa632393185",
            "2f4d3f02f4d4467c8adebd1af87e8d26",
            "9aff14be31fb4cff8ea4e794bb986420",
            "50ca4feff2164517b48142c6816c33dc",
            "a992c23cac2c49ddb8419b917e453193",
            "1f4b06730a1b457a88264b2fe22979e6",
            "73a6ba8a063a4b4ea2a092aabb2194fb",
            "73533e50f8be47feac9eda793dffd2d9",
            "1251e0a5a55649168288c748ae9bb0e4",
            "06648a235a6d4c68a5c55a3bdc4b7cc4",
            "9e0d198f6b42481db042440bdad905b1",
            "f187f5dd1ab8447c977a8b1350333520",
            "b5a19ba31c354483a4fce6477ae12279",
            "1d34dbf504df441385a4ed50b7a6be64",
            "6c0c35d261aa4df58e67e16be1eb43ca",
            "c3a1c544c0f8444d8afc02f5249b669a",
            "d2f04a0cd70f4f98a1906c830852e38d",
            "0774da51e7a94deb93403c9d8ad93703",
            "a6bad929a2784abdba2de3cfe444a70f",
            "1362485bdb6a45e68a6003899ae4ae50",
            "71f7765964954685af7cfc92d7ed7d16",
            "74954ed048ef4701a01784571bd49327",
            "6e435252131a4e228d127c4209b77f73",
            "8764b09c36384b82b03e388ce3b950bb",
            "061dacc5e810422587c2fa57d5f94dac",
            "e274f8c8b5134a63845d9567cbe357f4",
            "88301fc88be64f6ebe6f5aa6d99610f3",
            "c74682326f964a20bc1061b18fe55839",
            "a184b5ede45d4d6baa5610297ded3863",
            "6f0dfc3ae0304086a811e4b350c02d41",
            "f346e8af766743f0b68c5c566c65d1a7",
            "970decfecc4a4191872033574e0ea3a7",
            "88695ab3693e4fd6b500faffc0ea5bea"
          ]
        },
        "collapsed": true,
        "id": "Z0ZTYQ3Rq2Ub",
        "outputId": "71595761-116a-4967-f439-2b8d83bbc6bd"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "# 3. Load a dataset\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:100]\") # First 100 rows\n",
        "\n",
        "# 4. Set Training Arguments\n",
        "training_arguments = SFTConfig(\n",
        "output_dir = \"./phi2-8bit-finetuned\",\n",
        "dataset_text_field=\"text\",\n",
        "num_train_epochs=2,\n",
        "per_device_train_batch_size=2,\n",
        "gradient_accumulation_steps=4,\n",
        "# optimizer\n",
        "optim=\"adamw_8bit\",\n",
        "# Learning rate settings\n",
        "learning_rate=2e-4,\n",
        "lr_scheduler_type=\"cosine\",\n",
        "# logging\n",
        "logging_steps=10,\n",
        "report_to=\"none\",\n",
        "# Saving\n",
        "save_strategy=\"epoch\",\n",
        "# Precision settings (Auto detection)\n",
        "bf16=use_bf16,\n",
        "fp16=use_fp16,\n",
        "max_grad_norm=0.3, # max gradient norm based on QLoRA paper\n",
        "warmup_steps=100, # warmup_steps=0.03 used before based on QLoRA paper (now deprecated)\n",
        "# Memory optimization\n",
        "gradient_checkpointing=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488,
          "referenced_widgets": [
            "6d56b0bc5cad4d9abea779f7dde8caa4",
            "e514c0779d3040ccbc7dc88b3a1ebab8",
            "ce7cfba0599342929449f7dbe12a6169",
            "39d410d698074cbabb9325c8d80e43e2",
            "d90e52bfd0e84c8a960b5222b107bce1",
            "692785978cab4d20bf37da21d9dee3a1",
            "b883eca0ffb84b9db7d2692ef7f48678",
            "fdca0507d7ad44b79495d39c90cca930",
            "fa305643adaf492ea0666777470b0dee",
            "6d2bb16d34974bcfb09281593f3c9b83",
            "81d3e4fd520749fda4cf4e5db27ecec4",
            "4d86fa5a7e7d495c954d8a95c5327e2a",
            "d0c3a62fe6984d28bc779eff18bc2273",
            "9516045097dc4dea9f263c781e2fbd19",
            "58b372e6db354d41a0afcf27d6f4a267",
            "a8e0c6aaad1f491ca6eef2afb4efa2fb",
            "b906c51fc4314cac949d32d19469bcc1",
            "a680b8e303d447ffba9b134876bcd39a",
            "83b13c38feb745d09378ae43beafe2b9",
            "8dd336dbd2224058b7bfb7bb3f2cebd2",
            "8e3edfd44b0442c287f8e3c6062e6409",
            "7ca42db7c8e742b4a473e4346e2a8ef3",
            "4bc6c4c85b1047a883a025b43e950722",
            "040abf9010fd41edbfa1eb696966685d",
            "c3b8c96875c94f368b0ebb28a99a84a9",
            "378f81e15c60464ca6d51ffa06d8f79b",
            "9dc145f6ef8f46b4b397fe9ac56d01a2",
            "68039118c6484db09e8327980e0156d9",
            "74dcf424c85b4ff09772e1da1ca3f6b5",
            "bb8427acfbc44f7d81fe41fb282324a8",
            "babeef8cca0f4aba84edecd3e450c4e6",
            "311d4b54e3194179a8615c6e5185c6a5",
            "7b43d2c1253b4a608b47b5e37ec156d9"
          ]
        },
        "collapsed": true,
        "id": "b-9LqtjzhHMN",
        "outputId": "d3cb390b-ab13-41c9-bef6-b791ac8f5fd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "/home/itoua/anaconda3/envs/myenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/itoua/anaconda3/envs/myenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26/26 01:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.930900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.764000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/itoua/anaconda3/envs/myenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/itoua/anaconda3/envs/myenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=26, training_loss=1.8225610439593976, metrics={'train_runtime': 93.6382, 'train_samples_per_second': 2.136, 'train_steps_per_second': 0.278, 'total_flos': 485357902233600.0, 'train_loss': 1.8225610439593976, 'entropy': 1.7890512347221375, 'num_tokens': 23722.0, 'mean_token_accuracy': 0.586806442249905, 'epoch': 2.0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Initialize Trainer (SFTTrainer handles prompt formatting automatically)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    #peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# 6. Start Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0iMq0DlQvkO_",
        "outputId": "2e8496fc-76e2-487e-c5fb-d98e5d83e86c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./phi2-8bit-finetuned/tokenizer_config.json',\n",
              " './phi2-8bit-finetuned/special_tokens_map.json',\n",
              " './phi2-8bit-finetuned/vocab.json',\n",
              " './phi2-8bit-finetuned/merges.txt',\n",
              " './phi2-8bit-finetuned/added_tokens.json',\n",
              " './phi2-8bit-finetuned/tokenizer.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7. save the adapters (the LoRA weights)\n",
        "output_dir = \"./phi2-8bit-finetuned\"\n",
        "# Save the adapter weights\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "# Save the tokenizer (for exact padding/EOS settings)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_zRKASSynRE"
      },
      "source": [
        "#### Part 3: Reload the model in 8-bit, then attach the saved adapters for inference (after training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "22d2521e1b244a26a28294ba7a9e1617",
            "b1f505f1ad974c6587a0cc1a1d3a59de",
            "02db44de3ebf44cf8f53ecfde3036831",
            "cf0eadfcc8f744809ab469dd7e6c5a40",
            "909ab50b618a4f57960c6ce6e5c8295b",
            "fe16378d54d046e8883184f856e9527e",
            "01cc283d34b14587ab9b4d3142e8f82c",
            "0318b057ac4b48d48a3af246ba5de51a",
            "6afbfb3c43074a02a94616840d42f6b1",
            "ca1f04ea994648cda2133e96205e4a18",
            "05cb9f99dc61423999e31a46d9cd6f15"
          ]
        },
        "collapsed": true,
        "id": "hXi_jkitwEv2",
        "outputId": "e3d894c3-57bb-4bd3-e12d-10b760ae6ecc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:11<00:00,  5.67s/it]\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "model_id = \"microsoft/phi-2\"\n",
        "adapter_path = \"./phi2-8bit-finetuned\"\n",
        "\n",
        "# Load the weights in 8-bit integers again (not necessary already done in part I)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")\n",
        "# 1. Load the base model again (Frozen, 8-bit)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 2. Load and attach the saved adapters\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9hRCE3GvuMj1",
        "outputId": "5c141d62-9d39-44d2-a73d-801532252b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruct: Write a Python function to calculate the Fibonacci sequence.\n",
            "Output: def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Prepare the input\n",
        "prompt = \"Instruct: Write a Python function to calculate the Fibonacci sequence.\\nOutput:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# 4. Generate\n",
        "# We use torch.no_grad() to save memory during inference\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# 5. Decode output\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}